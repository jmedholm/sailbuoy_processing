{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a065a9d-7e8d-4f76-94ba-2e64ff274756",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.dates as mdates\n",
    "import gsw\n",
    "import glidertools as gt\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "import geomag\n",
    "from cmocean import cm as cmo\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib as mpl\n",
    "import cartopy.crs as ccrs\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,AutoMinorLocator)\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as mticker\n",
    "import sys\n",
    "from datetime import date\n",
    "\n",
    "sys.path.append('/Volumes/MASSIVEUNIT/Work/SOCHIC_2022/sailbuoy_processing/code/')\n",
    "\n",
    "%aimport dl_tools\n",
    "\n",
    "font = {'family' : 'Avenir',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 30}\n",
    "\n",
    "mpl.rc('font', **font)\n",
    "\n",
    "mpl.rcParams['xtick.major.size'] = 10\n",
    "mpl.rcParams['xtick.major.width'] = 2\n",
    "mpl.rcParams['xtick.minor.size'] = 6\n",
    "mpl.rcParams['xtick.minor.width'] = 1\n",
    "\n",
    "def rot_ticks(axs,rot,ha):\n",
    "    for xlabels in axs.get_xticklabels():\n",
    "                xlabels.set_rotation(rot)\n",
    "                xlabels.set_ha(ha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449436a1-d818-4021-94e6-e0cc71b62896",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SB Kringla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbec206-27b8-4145-9eff-651b18242654",
   "metadata": {},
   "source": [
    "Load and prep the data.txt from the datalogger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96b10766-76b5-40de-863e-2098c2bd494e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0823f837a445f1b2079e3a3c8aefeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checking lines in input file:   0%|          | 0/20213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20208 good measurements found in input file\n",
      "5 bad lines found in input file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = '/Volumes/MASSIVEUNIT/Work/SOCHIC_2022/Sailbuoy/data/DATA.TXT'\n",
    "\n",
    "ds = dl_tools.load_data(path)                                        # Loading the data\n",
    "ds = dl_tools.fix_standard_attrs(ds)                                 # Setting attributes and names for the standard variables\n",
    "ds = dl_tools.fix_airmar(ds)                                         # Setting attributes and names for the Airmar variables\n",
    "ds = dl_tools.fix_dcps(ds)                                           # Setting attributes and names for the DCPS variables\n",
    "ds = dl_tools.fix_aadi(ds)                                           # Setting attributes and names for the AADI Conductivity variables\n",
    "ds = ds.sortby('time').sel(time=slice('2022-01-10','2022-07-17'))    # Sorting in time, and selecting the time for the mission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4cd9f4-753d-4868-913b-5b40ee92b967",
   "metadata": {},
   "source": [
    "#### Assigning the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bed25c1f-1340-40c1-8017-6e2268b760b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.attrs['title']                   = 'SB Kringla Datalogger'\n",
    "ds.attrs['summary']                 = 'Datalogger output from Sailbuoy Kringla'\n",
    "ds.attrs['conventions']             = 'CF-1.6, ACDD-1.3'\n",
    "ds.attrs['creator_name']            = 'Johan Edholm'\n",
    "ds.attrs['platform']                = 'Uncrewed surface vehicle'\n",
    "ds.attrs['instrument_name']         = 'Kringla'\n",
    "ds.attrs['instrument_id']           = '1812'\n",
    "ds.attrs['institution']             = 'University of Gothenburg'\n",
    "ds.attrs['creator_email']           = 'johan.edholm@gu.se'\n",
    "ds.attrs['area']                    = 'Southern Ocean'\n",
    "ds.attrs['project']                 = 'SO-CHIC'\n",
    "ds.attrs['id']                      = 'SB Kringla Datalogger SO-CHIC'\n",
    "ds.attrs['instrument']              = 'Offshore Sensing AS Sailbuoy'\n",
    "ds.attrs['owner']                   = 'University of Gothenburg'\n",
    "ds.attrs['contact']                 = 'sebastiaan.swart@marine.gu.se'\n",
    "ds.attrs['processing_date']         = str(date.today())\n",
    "ds.attrs['processing_level']        = '2'\n",
    "ds.attrs['time_coverate_start']     = str(ds.time[0].values)\n",
    "ds.attrs['time_coverate_end']       = str(ds.time[-1].values)\n",
    "ds.attrs['geospatial_lat_min']      = str(ds.latitude.min().values)\n",
    "ds.attrs['geospatial_lat_max']      = str(ds.latitude.max().values)\n",
    "ds.attrs['geospatial_lon_min']      = str(ds.longitude.min().values)\n",
    "ds.attrs['geospatial_lon_max']      = str(ds.longitude.max().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30d2b13f-5910-420e-8cb2-4d7242cb5e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_netcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mformat\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgroup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0munlimited_dims\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcompute\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minvalid_netcdf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Delayed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Write dataset contents to a netCDF file.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "path : str, path-like or file-like, optional\n",
       "    Path to which to save this dataset. File-like objects are only\n",
       "    supported by the scipy engine. If no path is provided, this\n",
       "    function returns the resulting netCDF file as bytes; in this case,\n",
       "    we need to use scipy, which does not support netCDF version 4 (the\n",
       "    default format becomes NETCDF3_64BIT).\n",
       "mode : {\"w\", \"a\"}, default: \"w\"\n",
       "    Write ('w') or append ('a') mode. If mode='w', any existing file at\n",
       "    this location will be overwritten. If mode='a', existing variables\n",
       "    will be overwritten.\n",
       "format : {\"NETCDF4\", \"NETCDF4_CLASSIC\", \"NETCDF3_64BIT\",                   \"NETCDF3_CLASSIC\"}, optional\n",
       "    File format for the resulting netCDF file:\n",
       "\n",
       "    * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API\n",
       "      features.\n",
       "    * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only\n",
       "      netCDF 3 compatible API features.\n",
       "    * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,\n",
       "      which fully supports 2+ GB files, but is only compatible with\n",
       "      clients linked against netCDF version 3.6.0 or later.\n",
       "    * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not\n",
       "      handle 2+ GB files very well.\n",
       "\n",
       "    All formats are supported by the netCDF4-python library.\n",
       "    scipy.io.netcdf only supports the last two formats.\n",
       "\n",
       "    The default format is NETCDF4 if you are saving a file to disk and\n",
       "    have the netCDF4-python library available. Otherwise, xarray falls\n",
       "    back to using scipy to write netCDF files and defaults to the\n",
       "    NETCDF3_64BIT format (scipy does not support netCDF4).\n",
       "group : str, optional\n",
       "    Path to the netCDF4 group in the given file to open (only works for\n",
       "    format='NETCDF4'). The group(s) will be created if necessary.\n",
       "engine : {\"netcdf4\", \"scipy\", \"h5netcdf\"}, optional\n",
       "    Engine to use when writing netCDF files. If not provided, the\n",
       "    default engine is chosen based on available dependencies, with a\n",
       "    preference for 'netcdf4' if writing to a file on disk.\n",
       "encoding : dict, optional\n",
       "    Nested dictionary with variable names as keys and dictionaries of\n",
       "    variable specific encodings as values, e.g.,\n",
       "    ``{\"my_variable\": {\"dtype\": \"int16\", \"scale_factor\": 0.1,\n",
       "    \"zlib\": True}, ...}``\n",
       "\n",
       "    The `h5netcdf` engine supports both the NetCDF4-style compression\n",
       "    encoding parameters ``{\"zlib\": True, \"complevel\": 9}`` and the h5py\n",
       "    ones ``{\"compression\": \"gzip\", \"compression_opts\": 9}``.\n",
       "    This allows using any compression plugin installed in the HDF5\n",
       "    library, e.g. LZF.\n",
       "\n",
       "unlimited_dims : iterable of hashable, optional\n",
       "    Dimension(s) that should be serialized as unlimited dimensions.\n",
       "    By default, no dimensions are treated as unlimited dimensions.\n",
       "    Note that unlimited_dims may also be set via\n",
       "    ``dataset.encoding[\"unlimited_dims\"]``.\n",
       "compute: bool, default: True\n",
       "    If true compute immediately, otherwise return a\n",
       "    ``dask.delayed.Delayed`` object that can be computed later.\n",
       "invalid_netcdf: bool, default: False\n",
       "    Only valid along with ``engine=\"h5netcdf\"``. If True, allow writing\n",
       "    hdf5 files which are invalid netcdf as described in\n",
       "    https://github.com/shoyer/h5netcdf.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/opt/anaconda3/envs/sailbuoy_processing/lib/python3.10/site-packages/xarray/core/dataset.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds.to_netcdf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2606178-0aae-4068-ae61-d5877db81c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_netcdf('/Volumes/MASSIVEUNIT/Work/SOCHIC_2022/Sailbuoy/data/data.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c445672-d313-46e0-a523-62d2d4015b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [slice('2022-04-05 12:00','2022-05-03 12:00'),\n",
    "         slice('2022-05-03 12:00','2022-05-23 14:43:48'),\n",
    "         slice('2022-05-23 14:43:48','2022-06-17 07:23:22'),\n",
    "         slice('2022-06-17 07:23:22','2022-07-17 14:27')]\n",
    "\n",
    "files = ['../figs/transit_data_panels.png',\n",
    "         '../figs/vm_data_panels.png',\n",
    "         '../figs/SBoundary_data_panels.png',\n",
    "         '../figs/675_transect_data_panels.png']\n",
    "\n",
    "titles = ['Transit, 65-57Â°S',\n",
    "         'Virtual mooring',\n",
    "         'Crossing Southern Boundary',\n",
    "         'Transect with 675']\n",
    "\n",
    "vmaxs = [0.12,0.1,0.15,0.15]\n",
    "\n",
    "for i in range(4):\n",
    "    tmp = ds.sel(time=times[i])\n",
    "    plotting.NE_panels(ds = tmp,\n",
    "                       var = ['c_v_corr','c_u_corr','total_shear'],\n",
    "                       vmin = [-0.75,-0.75,0],\n",
    "                       vmax = [0.75,0.75,vmaxs[i]],\n",
    "                       cmap = [cmo.balance,cmo.balance,cmo.speed],\n",
    "                       ylim = (40,0),\n",
    "                       path = files[i],\n",
    "                       title = titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40c93db5-4c9a-42ef-91ec-db396ba5ed35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_netcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mformat\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgroup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0munlimited_dims\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcompute\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minvalid_netcdf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Delayed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Write dataset contents to a netCDF file.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "path : str, path-like or file-like, optional\n",
       "    Path to which to save this dataset. File-like objects are only\n",
       "    supported by the scipy engine. If no path is provided, this\n",
       "    function returns the resulting netCDF file as bytes; in this case,\n",
       "    we need to use scipy, which does not support netCDF version 4 (the\n",
       "    default format becomes NETCDF3_64BIT).\n",
       "mode : {\"w\", \"a\"}, default: \"w\"\n",
       "    Write ('w') or append ('a') mode. If mode='w', any existing file at\n",
       "    this location will be overwritten. If mode='a', existing variables\n",
       "    will be overwritten.\n",
       "format : {\"NETCDF4\", \"NETCDF4_CLASSIC\", \"NETCDF3_64BIT\",                   \"NETCDF3_CLASSIC\"}, optional\n",
       "    File format for the resulting netCDF file:\n",
       "\n",
       "    * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API\n",
       "      features.\n",
       "    * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only\n",
       "      netCDF 3 compatible API features.\n",
       "    * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,\n",
       "      which fully supports 2+ GB files, but is only compatible with\n",
       "      clients linked against netCDF version 3.6.0 or later.\n",
       "    * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not\n",
       "      handle 2+ GB files very well.\n",
       "\n",
       "    All formats are supported by the netCDF4-python library.\n",
       "    scipy.io.netcdf only supports the last two formats.\n",
       "\n",
       "    The default format is NETCDF4 if you are saving a file to disk and\n",
       "    have the netCDF4-python library available. Otherwise, xarray falls\n",
       "    back to using scipy to write netCDF files and defaults to the\n",
       "    NETCDF3_64BIT format (scipy does not support netCDF4).\n",
       "group : str, optional\n",
       "    Path to the netCDF4 group in the given file to open (only works for\n",
       "    format='NETCDF4'). The group(s) will be created if necessary.\n",
       "engine : {\"netcdf4\", \"scipy\", \"h5netcdf\"}, optional\n",
       "    Engine to use when writing netCDF files. If not provided, the\n",
       "    default engine is chosen based on available dependencies, with a\n",
       "    preference for 'netcdf4' if writing to a file on disk.\n",
       "encoding : dict, optional\n",
       "    Nested dictionary with variable names as keys and dictionaries of\n",
       "    variable specific encodings as values, e.g.,\n",
       "    ``{\"my_variable\": {\"dtype\": \"int16\", \"scale_factor\": 0.1,\n",
       "    \"zlib\": True}, ...}``\n",
       "\n",
       "    The `h5netcdf` engine supports both the NetCDF4-style compression\n",
       "    encoding parameters ``{\"zlib\": True, \"complevel\": 9}`` and the h5py\n",
       "    ones ``{\"compression\": \"gzip\", \"compression_opts\": 9}``.\n",
       "    This allows using any compression plugin installed in the HDF5\n",
       "    library, e.g. LZF.\n",
       "\n",
       "unlimited_dims : iterable of hashable, optional\n",
       "    Dimension(s) that should be serialized as unlimited dimensions.\n",
       "    By default, no dimensions are treated as unlimited dimensions.\n",
       "    Note that unlimited_dims may also be set via\n",
       "    ``dataset.encoding[\"unlimited_dims\"]``.\n",
       "compute: bool, default: True\n",
       "    If true compute immediately, otherwise return a\n",
       "    ``dask.delayed.Delayed`` object that can be computed later.\n",
       "invalid_netcdf: bool, default: False\n",
       "    Only valid along with ``engine=\"h5netcdf\"``. If True, allow writing\n",
       "    hdf5 files which are invalid netcdf as described in\n",
       "    https://github.com/shoyer/h5netcdf.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/opt/anaconda3/envs/sailbuoy_processing/lib/python3.10/site-packages/xarray/core/dataset.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds.to_netcdf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cc369f-4959-4fa0-bdf2-217f6863f0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
